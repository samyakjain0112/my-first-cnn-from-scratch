{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot\n",
    "import tensorflow\n",
    "import keras\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumed that a_train is the set of all images in the form of numpy array of dimensions total_no_of_images*height_of_each_image*width_of_each_image*no.of_layers_of_each_image\n",
    "#let us assume that the total no. of images are 50000\n",
    "#so let batch_size=sqrt(5000) that is around 225\n",
    "#so number of times we need to give in input for one epoch is 222\n",
    "#let the number of epochs be 20\n",
    "#let the image be 32*32*3\n",
    "batch_size=225\n",
    "n_epochs=20\n",
    "alpha=0.05\n",
    "#NOTE:\n",
    "#WE CAN RUN THE WHOLE ALGORITHM \n",
    "#as alpha=np.pow(10,-np.random.randn(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-94c520721021>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minitial_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m222\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0minput_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minitial_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0minitial_size\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_layer_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mweight_layer_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_layer_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a_data' is not defined"
     ]
    }
   ],
   "source": [
    "#FORWARD PROPOGATION\n",
    "initial_size=0\n",
    "for i in range(222):\n",
    "    input_batch=a_data[initial_size:initial_size+batch_size]\n",
    "    batch_layer_input=input_batch\n",
    "    weight_layer_1=np.random.randint(0,20,(8,len(batch_layer_1[0]),5,5))\n",
    "    batch_layer_input_padded=padding(batch_layer=batch_layer_input,size=1)\n",
    "    unactivated_output_1=multiplication(batch_layer=batch_layer_input_padded,weight_layer=weight_layer_1,strides=1)\n",
    "    activated_output_1=activation(unactivated_input=unactivated_output_1,activation='relu')\n",
    "    batch_layer_pool_1=pooling(activated_input=activated_output_1,typ='max',strides=2,height=3,width=3)\n",
    "    weight_layer_2=np.random.randint(0,20,(16,len(batch_layer_1[0]),5,5))\n",
    "    batch_layer_padded_1=padding(batch_layer=batch_layer_pool_1,size=1)\n",
    "    unactivated_output_2=multiplication(batch_layer=batch_layer_padded_1,weight_layer=weight_layer_2,strides=1)\n",
    "    activated_output_2=activation(unactivated_input=unactivated_output_2,activation='relu')\n",
    "    batch_layer_unroll=pooling(activated_input=activated_output_2,typ='max',strides=2,height=3,width=3)\n",
    "    fully_connected_1=unroll(unroller=batch_layer_unroll)\n",
    "    weight_fully_connected_1=np.random.randn(len(fully_connected_1[0]),40)\n",
    "    fully_connected_2=activate(layer=multipy_fully_connected(layer=fully_connected_1,weights=weight_fully_connected_1),type='relu')\n",
    "    weight_fully_connected_2=np.random.randn(len(fully_connected_2[0]),10)\n",
    "    fully_connected_3=activate(layer=multipy_fully_connected(layer=fully_connected_2,weights=weight_fully_connected_2),type='relu')\n",
    "    prediction=softmax(layer=fully_connected_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE IN CASE WE OBSERVE GRADIENT VANISHING OR GRADIENT BOOSTING PROBLEM THEN WE CAN MULTIPLY THE WEIGHTS BY math.sqrt(2/no.of weights ie len(weights[0]) for fully connected and len(weights[0][0])*len(weights[0][0][0]) for convulational layer))\n",
    "#the above was for relu activation but for sigmoid and tanh multiply by math.sqrt(1/no.of weights ie len(weights[0]) for fully connected and len(weights[0][0])*len(weights[0][0][0]) for convulational layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[7 7 7 ... 5 5 7]\n",
      "   [7 5 8 ... 7 6 5]\n",
      "   [7 7 6 ... 8 7 5]\n",
      "   ...\n",
      "   [8 7 5 ... 8 8 7]\n",
      "   [5 8 8 ... 5 5 8]\n",
      "   [5 5 8 ... 5 7 6]]\n",
      "\n",
      "  [[5 8 6 ... 7 8 5]\n",
      "   [7 6 5 ... 6 6 5]\n",
      "   [6 6 8 ... 6 5 6]\n",
      "   ...\n",
      "   [6 8 5 ... 8 8 7]\n",
      "   [8 5 5 ... 8 7 7]\n",
      "   [6 8 8 ... 8 5 7]]\n",
      "\n",
      "  [[8 5 5 ... 5 7 5]\n",
      "   [6 8 7 ... 5 5 6]\n",
      "   [5 8 8 ... 6 8 7]\n",
      "   ...\n",
      "   [5 5 6 ... 6 7 7]\n",
      "   [6 6 6 ... 6 5 6]\n",
      "   [7 5 8 ... 7 5 6]]]\n",
      "\n",
      "\n",
      " [[[7 5 8 ... 7 5 5]\n",
      "   [7 5 5 ... 5 8 8]\n",
      "   [6 8 6 ... 5 5 5]\n",
      "   ...\n",
      "   [6 7 8 ... 5 7 8]\n",
      "   [7 6 5 ... 8 5 8]\n",
      "   [5 7 6 ... 5 6 5]]\n",
      "\n",
      "  [[8 7 5 ... 6 5 8]\n",
      "   [5 7 6 ... 6 8 8]\n",
      "   [5 7 6 ... 6 8 6]\n",
      "   ...\n",
      "   [8 5 8 ... 6 8 8]\n",
      "   [5 6 8 ... 6 6 7]\n",
      "   [6 5 7 ... 7 7 8]]\n",
      "\n",
      "  [[7 5 5 ... 5 7 7]\n",
      "   [7 6 5 ... 5 6 8]\n",
      "   [5 6 5 ... 8 6 8]\n",
      "   ...\n",
      "   [8 5 6 ... 7 6 7]\n",
      "   [6 8 5 ... 8 7 6]\n",
      "   [6 6 5 ... 6 7 7]]]\n",
      "\n",
      "\n",
      " [[[6 6 7 ... 6 8 7]\n",
      "   [6 6 8 ... 8 6 6]\n",
      "   [5 7 6 ... 8 8 6]\n",
      "   ...\n",
      "   [7 6 6 ... 7 7 5]\n",
      "   [8 5 8 ... 7 7 6]\n",
      "   [5 5 8 ... 8 7 5]]\n",
      "\n",
      "  [[6 7 5 ... 5 8 7]\n",
      "   [6 6 6 ... 8 8 6]\n",
      "   [8 5 6 ... 6 6 5]\n",
      "   ...\n",
      "   [5 8 8 ... 7 8 8]\n",
      "   [7 5 7 ... 6 7 8]\n",
      "   [7 6 7 ... 7 7 6]]\n",
      "\n",
      "  [[6 5 6 ... 7 6 5]\n",
      "   [5 6 5 ... 8 8 7]\n",
      "   [6 6 7 ... 8 8 5]\n",
      "   ...\n",
      "   [6 5 5 ... 8 5 6]\n",
      "   [6 6 8 ... 6 7 8]\n",
      "   [5 8 8 ... 7 8 6]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[6 6 7 ... 6 5 8]\n",
      "   [8 6 5 ... 8 8 5]\n",
      "   [7 7 8 ... 7 7 8]\n",
      "   ...\n",
      "   [7 8 8 ... 6 5 6]\n",
      "   [5 5 8 ... 7 6 7]\n",
      "   [6 7 5 ... 6 8 7]]\n",
      "\n",
      "  [[6 7 8 ... 8 7 6]\n",
      "   [6 6 6 ... 5 5 7]\n",
      "   [7 8 7 ... 6 8 6]\n",
      "   ...\n",
      "   [5 6 6 ... 8 5 6]\n",
      "   [7 6 7 ... 6 6 8]\n",
      "   [5 5 7 ... 6 6 8]]\n",
      "\n",
      "  [[6 8 6 ... 7 5 5]\n",
      "   [5 8 5 ... 6 7 7]\n",
      "   [6 6 5 ... 6 8 7]\n",
      "   ...\n",
      "   [8 8 6 ... 6 6 5]\n",
      "   [8 6 5 ... 7 5 6]\n",
      "   [7 8 6 ... 5 6 7]]]\n",
      "\n",
      "\n",
      " [[[7 5 5 ... 6 7 7]\n",
      "   [8 5 8 ... 5 8 5]\n",
      "   [8 7 8 ... 7 6 6]\n",
      "   ...\n",
      "   [5 8 6 ... 7 6 8]\n",
      "   [6 5 5 ... 8 6 8]\n",
      "   [8 5 5 ... 7 5 7]]\n",
      "\n",
      "  [[7 8 6 ... 5 7 7]\n",
      "   [6 8 8 ... 6 6 5]\n",
      "   [5 6 8 ... 5 5 8]\n",
      "   ...\n",
      "   [7 8 6 ... 5 5 6]\n",
      "   [6 8 6 ... 7 8 7]\n",
      "   [5 6 6 ... 6 5 5]]\n",
      "\n",
      "  [[7 8 5 ... 6 7 5]\n",
      "   [8 8 5 ... 6 5 7]\n",
      "   [8 5 5 ... 6 8 8]\n",
      "   ...\n",
      "   [6 5 6 ... 5 5 7]\n",
      "   [5 7 6 ... 7 6 8]\n",
      "   [6 5 8 ... 5 5 7]]]\n",
      "\n",
      "\n",
      " [[[7 8 6 ... 6 7 8]\n",
      "   [7 5 8 ... 6 6 8]\n",
      "   [8 7 6 ... 6 5 5]\n",
      "   ...\n",
      "   [6 5 7 ... 8 8 6]\n",
      "   [5 7 5 ... 6 5 7]\n",
      "   [6 5 8 ... 6 6 5]]\n",
      "\n",
      "  [[8 5 6 ... 6 6 8]\n",
      "   [5 5 8 ... 8 5 8]\n",
      "   [7 8 5 ... 8 8 6]\n",
      "   ...\n",
      "   [7 5 8 ... 8 6 5]\n",
      "   [6 6 6 ... 5 5 8]\n",
      "   [7 5 5 ... 8 5 7]]\n",
      "\n",
      "  [[8 5 8 ... 8 5 8]\n",
      "   [8 8 7 ... 8 8 5]\n",
      "   [5 8 5 ... 8 5 8]\n",
      "   ...\n",
      "   [5 6 8 ... 7 6 5]\n",
      "   [6 7 6 ... 6 5 8]\n",
      "   [8 5 5 ... 6 5 7]]]]\n",
      "[[ 0.81089276  0.1231969  -0.24293965  2.2488154  -0.37767813 -0.37469761\n",
      "  -2.22772377 -0.82340921 -1.29568092]\n",
      " [-0.88264721 -0.02178058 -0.71183966 -0.02805397 -2.28742178  0.56505081\n",
      "   2.4372773   1.39994759 -1.44731939]\n",
      " [ 1.18444235  0.77243784  0.44033264 -1.07792979  0.52166334 -0.45950168\n",
      "   1.47653458 -0.2100577   0.21189328]\n",
      " [-0.2379343  -0.21164336  0.27127762 -0.89807844 -0.68245873  2.31152993\n",
      "  -1.19856938 -0.63624359  0.17264101]\n",
      " [-0.31277618 -0.33073072  0.93341028  0.79856046 -0.20190123 -0.93555498\n",
      "   1.57509809  0.09666117  0.10695047]]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.randint(5,9,(8,3,8,7)))\n",
    "print(np.random.randn(5,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(batch_layer,size):\n",
    "    for i in batch_layer:\n",
    "        for j in i:\n",
    "            for _ in range(size):\n",
    "                np.insert(j,0,np,zeros(len(j[0]),axis=1))\n",
    "                np.insert(j,len(j),np,zeros(len(j[0]),axis=1))\n",
    "                np.insert(j,0,np,zeros(len(j),axis=0))\n",
    "                np.insert(j,(len(j)),zeros(len(j),axis=0))\n",
    "    return batch_layer\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplication(batch_layer,weight_layer,strides=1):\n",
    "    unactivated_output=np.zeros([len(batch_layer),len(weight_layer),(len(batch_layer[0][0])-len(weight_layer[0][0]))/strides+1,(len(batch_layer[0][0])-len(weight_layer[0][0]))/strides+1])\n",
    "    counter=-1\n",
    "    filter_no=-1\n",
    "    for j in batch_layer:\n",
    "        counter+=1\n",
    "        for i in weight_layer:\n",
    "            filter_no+=1\n",
    "            for initial_index_y in range(0,len(j[0])-len(i[0])+1,strides):\n",
    "                for initial_index_x in range(0,len(j[0][0])-len(i[0][0])+1,strides):\n",
    "                    for k in range(len(i)):\n",
    "                        if initial_index_y+len(i[k])<=len(j[k]) & initial_index_x+len(i[k][0])<=len(j[k][0]):\n",
    "                            temp=j[k][initial_index_y:len(i[k])+initial_index_y]\n",
    "                            temp_final=[]\n",
    "                            for i in temp:\n",
    "                                temp_final.append(i[initial_index_x:len(i[k][0])+initial_index_x])\n",
    "                            temp_final=np.array(temp_final)\n",
    "                            unactivated_output[counter][filter_no][initial_index_y][initial_index_x]+=np.sum(np.dot(temp_final,i[k]))\n",
    "    return unactivated_output\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(unactivated_input,activation='relu'):\n",
    "    for i in range(len(unactivated_input)):\n",
    "        for j in range(len(unactivated_input[i])):\n",
    "            for k in range(len(unactivated_input[i][j])):\n",
    "                for l in range(len(unactivated_input[i][j][k])):\n",
    "                    if activation=='relu':\n",
    "                        if unactivated_input[i][j][k][l]<0:\n",
    "                            unactivated_input[i][j][k][l]=0\n",
    "                    elif activation=='sigmoid':\n",
    "                        unactivated_input[i][j][k][l]=sigmoid(unactivated_input[i][j][k][l])\n",
    "                    elif activation=='tanh':\n",
    "                        unactivated_input[i][j][k][l]=tanh(unactivated_input[i][j][k][l])\n",
    "    return unactivated_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling(activated_input,typ='max',strides=2,height=3,width=3):\n",
    "    base_layer=np.zeros([len(activated_input),len(activated_input[0]),(len(activated_input[0][0])-height)/strides+1,(len(activated_input[0][0][0])-width)/strides+1])\n",
    "    counter=-1\n",
    "    for j in activated_input:\n",
    "            counter+=1\n",
    "            for initial_index_y in range(0,len(j[0])-height+1,strides):\n",
    "                for initial_index_x in range(0,len(j[0][0])-width+1,strides):\n",
    "                    for k in range(len(j)):\n",
    "                        if initial_index_y+height<=len(j[k]) & initial_index_x+width<=len(j[k][0]):\n",
    "                            temp=j[k][initial_index_y:height+initial_index_y]\n",
    "                            temp_final=[]\n",
    "                            for i in temp:\n",
    "                                temp_final.append(i[initial_index_x:width+initial_index_x])\n",
    "                            temp_final=np.array(temp_final)\n",
    "                            if typ =='max':\n",
    "                                base_layer[counter][k][initial_index_y][initial_index_x]=np.max(temp_final)\n",
    "                            elif typ=='avg':\n",
    "                                base_layer[counter][k][initial_index_y][initial_index_x]=np.mean(temp_final)\n",
    "    return base_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll(unroller):\n",
    "    p=0\n",
    "    for i in unroller:\n",
    "        a=[]\n",
    "        for j in i:\n",
    "            for k in j:\n",
    "                for l in k:\n",
    "                    a.append(l)\n",
    "        if p==0:\n",
    "            c=np.array(a)\n",
    "            p=1\n",
    "        else:\n",
    "            np.insert(c,len(a),b,axis=1)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [1 2 4]\n",
      " [3 4 5]\n",
      " [6 7 8]]\n",
      "[[1 2 3]\n",
      " [3 4 5]\n",
      " [6 7 8]\n",
      " [1 2 4]]\n"
     ]
    }
   ],
   "source": [
    "#a=np.array([[1,2,3],[3,4,5],[6,7,8]])\n",
    "#b=np.array([[4,6,3],[9,4,2],[8,1,7]])\n",
    "\n",
    "#\n",
    "# IMPORTANT EXAMPLE FOR UNDERSTANDING\n",
    "#print(np.max(a))\n",
    "#in insert the insertion is done before the specified index so to insert at last write len(object)\n",
    "#c=np.array([])\n",
    "#w=np.array([1,2,4])\n",
    "#print(np.insert(a,1,w,axis=0))\n",
    "#a=np.insert(a,len(a),[1,2,4],axis=0)\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multipy_fully_connected(layer,weights):\n",
    "    return np.dot(layer,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(layer,type='relu'):\n",
    "    for i in range(len(layer)):\n",
    "        for j in range(len(layer[0])):\n",
    "            if type=='relu':\n",
    "                if layer[i][j]<0:\n",
    "                    layer[i][j]=0\n",
    "            elif type=='sigmoid':\n",
    "                layer[i][j]=sigmoid(layer[i][j])\n",
    "            elif type=='tanh':\n",
    "                layer[i][j]=tanh(layer[i][j])\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(layer,percentage,value):\n",
    "    for i in range(len(layer)):\n",
    "       \n",
    "        if value==1:#means drop out of convulational layer\n",
    "            for h in range(len(layer[i])):\n",
    "                b=unroll(layer[i][k])\n",
    "                k=np.random.randint(0,len(b),(percentage*len(b)))\n",
    "                for j in k:\n",
    "#considering square matrix of weights            \n",
    "                        hr=int(j/len(layer[i][h]))\n",
    "                        d=j%len(layer[i][h])\n",
    "                        layer[i][h][hr][d]=0\n",
    "        else:\n",
    "            k=np.random.randint(0,len(layer[i]),(percentage*len(layer[i])))\n",
    "            for j in k:\n",
    "                layer[i][j]=0\n",
    "    return layer\n",
    "    #also multiply all oyher values by 1(1-percentage/100) \\\n",
    "    #this i have not implemented in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.333333333333333\n"
     ]
    }
   ],
   "source": [
    "#a=np.array([[1,2,3],[3,4,5],[6,7,8]])\n",
    "#print(np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(data):\n",
    "    return 1/(1+math.exp(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(data):\n",
    "    return (math.exp(data)**2-1)/(math.exp(data)**2+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(layer):\n",
    "    s=0\n",
    "    for i in layer:\n",
    "        s+=math.exp(i)\n",
    "    for i in layer:\n",
    "        a.append(math.exp(i)/s)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************************************************************************************************************\n",
    "#******************************************************************************************************************************************\n",
    "#COMPLETED THE FOREWARD PROPOGATION NOW ITS TIME FOR BACKPROPOGATION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a_train_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-568c9b44a21b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#and the major_delta_fully_connected or major_delta_connected is (\"dz\") and costing_fully_connected or costing_connected is (\"dw\") similarly for major_delta_connected_poollayer_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#note here we have initiated backpropogation by assuming that the last activation function is either sigmoiod or softmax thats why we have direcly taken delta as y-a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_train_output\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmajor_delta_fully_connected_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#r=np.sum(fully_connected_2,axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a_train_output' is not defined"
     ]
    }
   ],
   "source": [
    "#BACKPROPOGATION\n",
    "#note the convention used we have used that minor_delta_fully_connected or minor_delta_connected is (\"da\"),\n",
    "#and the major_delta_fully_connected or major_delta_connected is (\"dz\") and costing_fully_connected or costing_connected is (\"dw\") similarly for major_delta_connected_poollayer_2\n",
    "#note here we have initiated backpropogation by assuming that the last activation function is either sigmoiod or softmax thats why we have direcly taken delta as y-a\n",
    "error=a_train_output-predictions\n",
    "major_delta_fully_connected_2=error\n",
    "#r=np.sum(fully_connected_2,axis=0)\n",
    "#r=r.reshape(len(r),1)\n",
    "costing_fully_connected_2=np.zeros(np.shape(weight_fully_connected_2))\n",
    "for i in range(len(fully_connected_2)):\n",
    "    costing_fully_connected_2+=fully_connected_2[i].T*major_delta_fully_connected_2[i]\n",
    "weight_fully_connected_2-=alpha*costing_fully_connected_2/(len(fully_connected_2))\n",
    "minor_delta_fully_connected_1=major_delta_fully_connectes_2*weight_fully_connected_2.T\n",
    "major_delta_fully_connected_1=minor_delta_fully_connected_1*derivate_activation(activation_used='relu',layer=fully_connected_2)\n",
    "costing_fully_connected_1=np.zeros(np.shape(weight_fully_connected_1))\n",
    "for i in range(len(fully_connected_1)):\n",
    "    costing_fully_connected_1+=fully_connected_1[i].T*major_delta_fully_connected_1[i]\n",
    "weight_fully_connected_1-=alpha*costing_fully_connected_1/(len(fully_connected_1))\n",
    "minor_delta_fully_connected_0=major_delta_fully_connectes_1*weight_fully_connected_1.T\n",
    "major_delta_fully_connected_0=minor_delta_fully_connected_0*derivate_activation(activation_used='relu',layer=fully_connected_1)\n",
    "major_delta_connected_poollayer_2=make_fully_connected_into_conv(delta_terms=major_delta_fully_connected_0,layer=batch_layer_unroll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-116-2a28deaa8e38>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-116-2a28deaa8e38>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def derivative_activation(activation_used='relu',weights,layer):\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def derivative_activation(activation_used='relu',layer):\n",
    "    if activation_used=='relu':\n",
    "        return 1\n",
    "    elif activation_used=='sigmoid':\n",
    "        return layer*(1-layer)\n",
    "    elif activationn_used=='tanh':\n",
    "        return 1-layer*layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fully_connected_into_conv(delta_terms,layer):\n",
    "    p=1\n",
    "    init=0\n",
    "    initial_conv=0\n",
    "    for i in range(len(layer)*len(layer[0])):\n",
    "        a=[]\n",
    "        init=initial_conv\n",
    "        for initial_index_convert in range(init,init+len(layer[0][0])*len(layer[0][0][0]),len(layer[0][0][0])):\n",
    "            initial_conv=initial_index_convert\n",
    "            a.append([delta_terms[initial_index_convert:len(layer[0][0][0])+initial_index_convert]])\n",
    "        if p==1:\n",
    "            arr=np.array(a)\n",
    "            p=0\n",
    "        else:\n",
    "            np.insert(arr,len(arr),a,axis=0)\n",
    "    arr=arr.reshape(len(layer),len(layer[0]),len(layer[0][0]),len(layer[0][0][0]))\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BACKPROPOGATION FOR CONVULATIONOAL AND POOLING LAYER\n",
    "major_delta_connected_convlayer_2=delta_pool_to_conv(activated_output=activated_output_2,batch_layer=batch_layer_unroll,major_delta_connected_poollayer=major_delta_connected_poollayer_2)\n",
    "costing_connected_2=cal_gradient_conv(layer_delta=major_delta_connected_convlayer_2,layer_false_weight=batch_layer_padded_1,weights=weight_layer_2)\n",
    "minor_delta_connected_poollayer_1=cal_delta_conv(weights=weight_layer_2,layer_delta=major_delta_connected_convlayer_2)\n",
    "major_delta_connected_poollayer_1=minor_delta_connected_poollayer_1*derivative_activation(activation_used='relu',layer=batch_layer_pool_1)\n",
    "major_delta_connected_convlayer_1=delta_pool_to_conv(activated_output=activated_output_1,batch_layer_pool=batch_layer_pool_1,major_delta_connected_poollayer=major_delta_connected_poollayer_1)\n",
    "costing_connected_1=cal_gradient_conv(layer_delta=major_delta_connected_convlayer_1,layer_false_weight=batch_layer_input_padded,weights=weight_layer_1)\n",
    "weight_layer_1-=alpha*costing_connected_1/len(batch_layer_pool_1)\n",
    "weight_layer_2-=alpha*costing_connected_2/len(batch_layer_unroll)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def delta_pool_to_conv(activated_output,batch_layer,major_delta_connected_poollayer):\n",
    "    dup_activated_output=np.zeros(np.shape(activated_output))\n",
    "    for i in range(len(batch_layer)):\n",
    "        for j in range(len(batch_layer[i])):\n",
    "            for k in range(batch_layer[i][j]):\n",
    "                for l in range(batch_layer[i][j][k]):\n",
    "                    for q in range(activated_output[i][j]):\n",
    "                        for w in range(activated_output[i][j][q]):\n",
    "                            if activated_output[i][j][q][w]==batch_layer[i][j][k][l]:\n",
    "                                dup_activated_output[i][j][q][w]=major_delta_connected_poollayer[i][j][k][l]\n",
    "    return dup_activated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_gradient_conv(layer_delta,layer_false_weight,weights) :\n",
    "    gradient=np.zeros(len(weight),len(layer_false_weight[0]),len(weights[0][0]),len(weights[0][0][0]))\n",
    "    t=len(layer_false_weight[0])\n",
    "    s=len(layer_delta)\n",
    "    y=len(layer_delta[0])#no. of filters\n",
    "    k=0\n",
    "    init=0\n",
    "    counter=0\n",
    "    for j in range(s):\n",
    "        for p in range(y):\n",
    "            init=k+1 \n",
    "            for i in range(init,init+t):\n",
    "                layer_delta=np.insert(layer_delta[j],i+1,layer_delta[j][i],axis=0)\n",
    "                k=k+1\n",
    "            gradient[counter%y]+=(multiplication(batch_layer=layer_false_weight[j],weight_layer=layer_delta[j][init:init+t],strides=1)\n",
    "    #strids should be same as the time of forward propogation\n",
    "            counter+=1\n",
    "return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  6]\n",
      " [ 6 10]]\n"
     ]
    }
   ],
   "source": [
    "d=np.array([[[1,2],[3,4]],[[1,4],[5,6]]])\n",
    "print(np.sum(d,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_delta_conv(weights,layer_delta):\n",
    "    #rotate weight vector by 180 degrees clockwise:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can implement ada booost for better results\n",
    "#here we also need to implement for b\n",
    "#note we can also implement the batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "[[10]\n",
      " [13]\n",
      " [16]]\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[1,2,3],[3,4,5],[6,7,8]])\n",
    "t=np.sum(a,axis=0)\n",
    "r=t.reshape(len(t),1)\n",
    "print(r.shape)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
